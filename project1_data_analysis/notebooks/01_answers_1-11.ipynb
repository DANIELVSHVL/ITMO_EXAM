{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyME93vv2riL3O4TVKteq4bb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZpr6-f9Oyi9","executionInfo":{"status":"ok","timestamp":1754749697894,"user_tz":-180,"elapsed":19440,"user":{"displayName":"Daniel Levashvili","userId":"18067212155029720823"}},"outputId":"7d7c3b8e-4ee2-41ef-e1b0-452044e7f117"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtbNIvBXN3CG","executionInfo":{"status":"ok","timestamp":1754749868975,"user_tz":-180,"elapsed":80888,"user":{"displayName":"Daniel Levashvili","userId":"18067212155029720823"}},"outputId":"47a47bb8-228c-48bd-e86e-f8551d1ce667"},"outputs":[{"output_type":"stream","name":"stdout","text":["1. 0.2\n","2. Russia,Mexico,Brazil,Nigeria,Australia\n","3. 2.4\n","4. 0.2\n","5. Unknown City\n","6. Unknown City\n","7. 460\n","8. 418\n","9. 875\n","10. 1350\n","11. 158\n"]}],"source":["import json\n","import numpy as np\n","import pandas as pd\n","from collections import Counter, defaultdict\n","import pyarrow.dataset as ds\n","\n","# ===== Пути =====\n","path_tx = \"/content/drive/MyDrive/ИТМО/transaction_fraud_data.parquet\"\n","path_fx = \"/content/drive/MyDrive/ИТМО/historical-currency-exchange.csv\"\n","\n","# ===== Курсы валют =====\n","fx = pd.read_csv(path_fx)\n","fx[\"date\"] = pd.to_datetime(fx[\"date\"], errors=\"coerce\").dt.date\n","fx_long = fx.melt(id_vars=\"date\", var_name=\"currency\", value_name=\"rate\").dropna(subset=[\"rate\"])\n","fx_long[\"currency\"] = fx_long[\"currency\"].str.upper()\n","fx_map = {(d, c): float(r) for d, c, r in fx_long.itertuples(index=False)}\n","\n","# ===== Аккумуляторы =====\n","fraud_cnt = rows_cnt = 0\n","fraud_by_country = Counter()\n","\n","sum_by_city = defaultdict(float)\n","cnt_by_city = defaultdict(int)\n","sum_ff_by_city = defaultdict(float)\n","cnt_ff_by_city = defaultdict(int)\n","\n","fr_high_num = fr_high_den = 0\n","\n","usd_non_fraud, usd_fraud = [], []\n","\n","sum_group_sizes = 0\n","n_groups = 0\n","\n","um_values_by_client = defaultdict(list)\n","\n","# ===== Какие колонки нам нужны =====\n","usecolumns = [\n","    \"is_fraud\",\"country\",\"customer_id\",\"timestamp\",\"is_high_risk_vendor\",\n","    \"city\",\"amount\",\"vendor_type\",\"currency\",\"last_hour_activity\",\"device\"\n","]\n","\n","# ===== Чтение по частям через pyarrow.dataset =====\n","dataset = ds.dataset(path_tx, format=\"parquet\")\n","\n","# Перебор чанков\n","for batch in dataset.to_batches(columns=usecolumns, batch_size=1_000_000):\n","    chunk = batch.to_pandas()\n","\n","    # Время\n","    chunk[\"ts\"] = pd.to_datetime(chunk[\"timestamp\"], errors=\"coerce\")\n","    chunk[\"h\"] = chunk[\"ts\"].dt.floor(\"h\")\n","\n","    # 1\n","    fraud_cnt += int(chunk[\"is_fraud\"].sum())\n","    rows_cnt  += len(chunk)\n","\n","    # 2\n","    fraud_by_country.update(chunk.loc[chunk[\"is_fraud\"] == 1, \"country\"].dropna().astype(str))\n","\n","    # 3\n","    sizes = chunk.groupby([\"customer_id\", \"h\"]).size()\n","    sum_group_sizes += int(sizes.sum())\n","    n_groups += int(sizes.shape[0])\n","\n","    # 4\n","    mask_hr = chunk[\"is_high_risk_vendor\"] == True\n","    fr_high_num += int(chunk.loc[mask_hr, \"is_fraud\"].sum())\n","    fr_high_den += int(mask_hr.sum())\n","\n","    # 5\n","    g = chunk.groupby(\"city\")[\"amount\"].agg(sum=\"sum\", count=\"size\")\n","    for city, row in g.iterrows():\n","        sum_by_city[city] += float(row[\"sum\"])\n","        cnt_by_city[city] += int(row[\"count\"])\n","\n","    # 6\n","    ff = chunk.loc[chunk[\"vendor_type\"] == \"fast_food\"].groupby(\"city\")[\"amount\"].agg(sum=\"sum\", count=\"size\")\n","    for city, row in ff.iterrows():\n","        sum_ff_by_city[city] += float(row[\"sum\"])\n","        cnt_ff_by_city[city] += int(row[\"count\"])\n","\n","    # 7–10: перевод в USD\n","    d = chunk[\"ts\"].dt.date\n","    rates = np.array([fx_map.get((di, ci), np.nan) for di, ci in zip(d, chunk[\"currency\"].str.upper())], dtype=float)\n","    valid = ~np.isnan(rates) & (rates != 0.0)\n","    if valid.any():\n","        amt_usd = chunk.loc[valid, \"amount\"].to_numpy(dtype=float) / rates[valid]\n","        nf = chunk.loc[valid, \"is_fraud\"] == 0\n","        fr = chunk.loc[valid, \"is_fraud\"] == 1\n","        usd_non_fraud.extend(amt_usd[nf])\n","        usd_fraud.extend(amt_usd[fr])\n","\n","    # 11\n","    sub = chunk[[\"customer_id\", \"h\", \"last_hour_activity\"]].dropna().drop_duplicates([\"customer_id\", \"h\"])\n","    for cid, lha in zip(sub[\"customer_id\"], sub[\"last_hour_activity\"]):\n","        try:\n","            obj = json.loads(lha) if isinstance(lha, str) else lha\n","            if isinstance(obj, dict) and \"unique_merchants\" in obj:\n","                um_values_by_client[cid].append(int(obj[\"unique_merchants\"]))\n","        except:\n","            pass\n","\n","# ===== Финализация =====\n","fraud_ratio_rounded = round(np.ceil((fraud_cnt / max(rows_cnt, 1)) * 10) / 10, 1)\n","top_keys = [c for c, _ in fraud_by_country.most_common(5) if isinstance(c, str) and c]\n","top5_fraud_countries = \",\".join(top_keys)\n","avg_tx_per_client_hour = round(np.floor((sum_group_sizes / max(n_groups, 1)) * 10) / 10, 1)\n","fraud_ratio_high_risk = round(np.ceil((fr_high_num / max(fr_high_den, 1)) * 10) / 10, 1)\n","city_highest_avg = max(sum_by_city, key=lambda k: sum_by_city[k] / max(cnt_by_city[k], 1)) if sum_by_city else None\n","city_fast_food_highest = max(sum_ff_by_city, key=lambda k: sum_ff_by_city[k] / max(cnt_ff_by_city[k], 1)) if sum_ff_by_city else None\n","usd_nf = pd.Series(usd_non_fraud, dtype=float).dropna()\n","usd_fr = pd.Series(usd_fraud, dtype=float).dropna()\n","mean_usd_non_fraud = int(np.ceil(usd_nf.mean())) if not usd_nf.empty else None\n","std_usd_non_fraud  = int(np.ceil(usd_nf.std(ddof=0))) if not usd_nf.empty else None\n","mean_usd_fraud     = int(np.ceil(usd_fr.mean())) if not usd_fr.empty else None\n","std_usd_fraud      = int(np.ceil(usd_fr.std(ddof=0))) if not usd_fr.empty else None\n","med_by_client = [np.median(v) for v in um_values_by_client.values() if v]\n","dangerous_clients_count = int(sum(m > np.quantile(med_by_client, 0.95) for m in med_by_client)) if med_by_client else 0\n","\n","# ===== Вывод =====\n","print(\"1.\", fraud_ratio_rounded)\n","print(\"2.\", top5_fraud_countries)\n","print(\"3.\", avg_tx_per_client_hour)\n","print(\"4.\", fraud_ratio_high_risk)\n","print(\"5.\", city_highest_avg)\n","print(\"6.\", city_fast_food_highest)\n","print(\"7.\", mean_usd_non_fraud)\n","print(\"8.\", std_usd_non_fraud)\n","print(\"9.\", mean_usd_fraud)\n","print(\"10.\", std_usd_fraud)\n","print(\"11.\", dangerous_clients_count)"]},{"cell_type":"code","source":["# === настройки ===\n","GH_USER = \"DANIELVSHVL\"\n","GH_REPO = \"ITMO_EXAM\"\n","BRANCH  = \"feature/data-analysis-answers\"   # имя новой ветки\n","\n","# путь к ТВОЕМУ ноутбуку с ответами 1–11\n","NB_SRC = \"/content/drive/MyDrive/Colab Notebooks/01_answers_1-11.ipynb\"\n","\n","from getpass import getpass\n","GH_TOKEN = getpass(\"GitHub token (ввод скрыт): \")\n","\n","# === код ===\n","import os, sys, shutil, pathlib, textwrap, subprocess\n","\n","repo_dir = \"/content/ITMO_EXAM\"\n","remote   = f\"https://{GH_USER}:{GH_TOKEN}@github.com/{GH_USER}/{GH_REPO}.git\"\n","\n","# начисто клонируем\n","if os.path.exists(repo_dir):\n","    shutil.rmtree(repo_dir)\n","subprocess.run([\"git\",\"clone\",remote,repo_dir], check=True)\n","\n","# структура\n","for p in [\n","    \"data\",\n","    \"project1_data_analysis/notebooks\",\n","    \"project1_data_analysis/src\",\n","    \"project2_chatbot\",\n","    \"reports/figures\",\n","]:\n","    pathlib.Path(f\"{repo_dir}/{p}\").mkdir(parents=True, exist_ok=True)\n","\n","# базовые файлы\n","(open(f\"{repo_dir}/.gitignore\",\"w\")\n"," .write(\"data/*\\n*.parquet\\n*.csv\\n.ipynb_checkpoints/\\n.DS_Store\\n\"))\n","(open(f\"{repo_dir}/requirements.txt\",\"w\")\n"," .write(\"pandas\\nnumpy\\npyarrow\\n\"))\n","(open(f\"{repo_dir}/README.md\",\"w\",encoding=\"utf-8\")\n"," .write(f\"# ITMO_EXAM\\n\\nДва проекта:\\n\\n- project1_data_analysis — анализ транзакций.\\n- project2_chatbot — чат‑бот (WIP).\\n\\nДанные не коммитятся. Python {sys.version.split()[0]}.\\n\"))\n","readme_p1 = \"\"\"# Project 1 – Data Analysis\n","\n","- notebooks/01_answers_1-11.ipynb — вычисление метрик и ответов 1–11.\n","- Данные читаются из Parquet, большие файлы не коммитим.\n","\n","Как запустить локально:\n","1) pip install -r ../../requirements.txt\n","2) jupyter notebook\n","\"\"\"\n","open(f\"{repo_dir}/project1_data_analysis/README.md\",\"w\",encoding=\"utf-8\").write(readme_p1)\n","\n","# копируем ноутбук\n","assert os.path.exists(NB_SRC), f\"Не найден ноутбук: {NB_SRC}\"\n","dst_nb = f\"{repo_dir}/project1_data_analysis/notebooks/01_answers_1-11.ipynb\"\n","shutil.copy2(NB_SRC, dst_nb)\n","\n","# git config + ensure main + push ветки\n","def sh(cmd):\n","    subprocess.run([\"bash\",\"-lc\",cmd], check=True)\n","\n","sh(f'cd \"{repo_dir}\" && git config user.name \"colab-commit\" && git config user.email \"colab-commit@example.com\"')\n","# если нет main — создадим и запушим\n","sh(f'cd \"{repo_dir}\" && (git rev-parse --verify main >/dev/null 2>&1 || (git checkout -b main && git commit --allow-empty -m \"Init main\" && git push -u origin main))')\n","# новая ветка + коммит\n","sh(f'cd \"{repo_dir}\" && git checkout -b \"{BRANCH}\" && git add . && git commit -m \"Add structure + 01_answers_1-11.ipynb + READMEs + requirements\" && git push -u origin \"{BRANCH}\"')\n","\n","print(\"OK — пуш выполнен.\")\n","print(f\"Создай PR: https://github.com/{GH_USER}/{GH_REPO}/compare/{BRANCH}?expand=1\")\n"],"metadata":{"id":"xf9dZBpwRjfg"},"execution_count":null,"outputs":[]}]}